<!--
AIâ€‘Chatâ€‘CLI â€¢ Codex Prompt
PhaseÂ 7 â€“ LLM Integration (Coverage Fix)
Save this file as docs/codex/phaseâ€‘7â€‘llm.md
Author: JamalÂ Alâ€‘Sarraf <jalsarraf0@gmail.com>
-->

# PhaseÂ 7 Prompt â€“ OpenAI Adapter & Coverage Boost ğŸ¤–
*Runner matrix: **Linuxâ€¯[selfâ€‘hosted] Â· Windowsâ€¯[selfâ€‘hosted] Â· macOSâ€¯(macosâ€‘latest)** â€” coverage gate **â‰¥â€¯90â€¯%***

---

## Mandatory tasks

1. **Add highâ€‘coverage unit tests** (`//go:build unit`) for `pkg/llm/openai` covering 5 edge cases via `httptest.Server`.
2. **Refactor openai client**: inject `sleep func(time.Duration)` for retry; production uses `time.Sleep`, tests use noâ€‘op.
3. **Create helper** `internal/testhttp/roundtrip.go` for transport injection.
4. **Add `embedutil` panic test**.
5. **Split Make targets**
   ```make
   unit:
go test -race -covermode=atomic -coverprofile=coverage.out -tags unit ./...
   test: lint unit
   ```
6. **macOS cache fix**: preâ€‘clean `${{ env.CACHE_DIR }}` before `actions/cache@v3` restore.
7. **Docs**: update `docs/llm.md` with â€œUnit vs Liveâ€ test section.
8. **Gate**: total coverage â‰¥â€¯90â€¯% on all OSes; live tests remain optâ€‘in (`OPENAI_API_KEY`).

---

## Acceptance criteria

* CI pipeline green; coverage gate passes.
* No cache restore errors on macOS.
* Commit signed **JamalÂ Alâ€‘Sarraf <jalsarraf0@gmail.com>**.

---

MIT Â©Â 2025Â JamalÂ Alâ€‘Sarraf
