<!--
AIâ€‘Chatâ€‘CLI â€¢ Codex Prompt
PhaseÂ 7 â€“ LLM Integration (OpenAI Adapter)
Save this file as docs/codex/phase-7-llm.md
Author: Jamal Alâ€‘Sarraf <jalsarraf0@gmail.com>
-->

# PhaseÂ 7 Prompt â€“ LLM Integration (OpenAI Adapter) ðŸ¤–
*Runner matrix: **Linuxâ€¯[selfâ€‘hosted,â€¯linux] Â· Windowsâ€¯[selfâ€‘hosted,â€¯windows] Â· macOSâ€¯(macosâ€‘latest)** â€” coverage gate **â‰¥â€¯88â€¯%** (expected ~90â€¯%).*

---

## Deliverables (must all be implemented)

| # | Component | Precise requirement |
|---|-----------|---------------------|
| 1 | **`pkg/llm` interface** | `Client.Completion(ctx, req)` â†’ streaming `Stream`. Structures defined exactly as spec. |
| 2 | **OpenAI backâ€‘end** (`llm/openai`) | Uses `/v1/chat/completions` with `stream=true`. Reads key from **`AI_CHAT_API_KEY`** or `config.yaml:api_key`. Base URL override `AICHAT_BASE_URL`. |
| 3 | **Mock backâ€‘end** (`llm/mock`) | Deterministic Stream for tests (`mock.New("hi", "there")`). |
| 4 | **TUI streaming** | Goroutine pumps tokens into BubbleÂ Tea via `llmTokenMsg`; spinner hides when complete. |
| 5 | **CLI wiring** | `ai-chat ask <prompt>` streams to stdout. Flags & config: `model`, `temperature`, `max_tokens`. |
| 6 | **Tests** | 100â€¯% in `llm/mock`; integration test in openai pkg skipped when `AI_CHAT_API_KEY` unset. TUI stream test validates spinner off. Project coverage â‰¥â€¯90â€¯%. |
| 7 | **Docs** | `docs/llm.md` + README flag table. |
| 8 | **Security** | Redact API key in logs, 30â€¯s timeout (`AICHAT_TIMEOUT` override). |

---

## Makefile additions

```make
live-openai-test:
go test ./pkg/llm/openai -run Live -v
```

---

## CI

No new jobs; unit tests use mock, so pipeline offlineâ€‘safe.

---

## Acceptance checklist

* CI green on Linux/Windows selfâ€‘hosted, macOSâ€‘latest.  
* `ai-chat ask` works with live key.  
* Coverage â‰¥â€¯88â€¯%.  
* Docs merged; commit signed **JamalÂ Alâ€‘Sarraf**.

---

MIT Â©Â 2025Â JamalÂ Alâ€‘Sarraf
